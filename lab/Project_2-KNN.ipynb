{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYn8_-z8VpNt"
      },
      "source": [
        "## Assignment – Build a classifier for detecting Mines and Determine Influence of k in K‑Nearest Neighbours on the UCI Sonar Dataset\n",
        "\n",
        " There are two classes - mine \"M\" (positive class, label 1) and rock \"R\" (negative class, label 0)\n",
        "\n",
        "**Learning goals**\n",
        "\n",
        "-- Detect and reason about class imbalance.\n",
        "\n",
        "-- Evaluate two -values (3 and 5) for K‑NN using proper cross‑validation.\n",
        "\n",
        "-- Compare accuracy to class‑sensitive metrics (precision, recall, F1).\n",
        "\n",
        "-- Practise writing result tables and interpreting trade‑offs.\n",
        "\n",
        "-- (Extension) Experiment with a third classifier of your choice.\n",
        "\n",
        "**Load dataset**\n",
        "use the following command to convert the string labels into categorical\n",
        "```\n",
        "y = df['Label'].astype('category').cat.codes\n",
        "```\n",
        "**Part 1  Exploratory Check**\n",
        "\n",
        "- Missing values  Report the count of NaN values in the raw CSV.\n",
        "\n",
        "- Class balance: (a) Display a bar plot of R vs M counts.\n",
        "\n",
        "(b) Compute the imbalance ratio (larger‑class ∕ smaller‑class).\n",
        "\n",
        "(c) Decide whether the dataset is balanced or imbalanced (justify). This is upto your discretion.\n",
        "\n",
        "\n",
        "\n",
        "**Part 2  Modelling**\n",
        "\n",
        "- Perform 80 / 20 train‑test split (stratified if imbalanced).\n",
        "\n",
        "- use StandardScaler → KNeighborsClassifier inside each fold.\n",
        "\n",
        "- 5‑fold CV (use StratifiedKFold when imbalanced, plain KFold otherwise).\n",
        "\n",
        "- Evaluate two models: Model  (a) KNN n_neighbors = 3 (b) KNN n_neighbors = 5\n",
        "\n",
        "- For each model record:\n",
        "\n",
        "i) Mean training accuracy (across folds)\n",
        "\n",
        "ii) Mean validation accuracy\n",
        "\n",
        "iii) Gap = train − val\n",
        "\n",
        "- Test accuracy on the held‑out 20 % split\n",
        "\n",
        "- Confusion matrix on the test set\n",
        "\n",
        "- Precision, recall, F1  \n",
        "\n",
        "Fill the required metrics into the table template below (extend it with the new columns for precision/recall/F1):\n",
        "\n",
        "| k | Train Acc | Val Acc | Gap | Test Acc | Precision | Recall | F1 | Confusion Matrix | Best |\n",
        "| :-: | :-------: | :-----: | :--: | :------: | :-------: | :----: | :---: | :--------------- | :--: |\n",
        "\n",
        "Q1: State the best model whichever model you would deploy if the main goal is to minimise missed mines (maximise recall).\n",
        "\n",
        "Make your judgement using the following reasong - If Based on your validation accuracy for k = 3 looked best, but the hold-out test shows k = 5 has higher balanced accuracy. Which would you deploy given that missing a mine carries greater risk than a false alarm?”\n",
        "\n",
        "**Part 3  Interpretation Questions**\n",
        "\n",
        "Q2: Class imbalance – How severe is it, and which metric (precision vs recall) carries more significance in a sonar mine‑hunting scenario?\n",
        "\n",
        "Q3: k = 3 vs k=5 – Compare validation vs test results. Which k generalises better?\n",
        "\n",
        "Q4:   Error profile – Using the confusion matrices, state whether false positives or false negatives dominate and discuss practical impact.\n",
        "\n",
        "**Part 4  Extension (Open‑Ended)**\n",
        "\n",
        "- Pick one additional classifier such as SVC:\n",
        "\n",
        "- Evaluate it with the same pipeline and metrics (include precision/recall/F1).\n",
        "\n",
        "- Add a new row to the results table.\n",
        "\n",
        "- State whether it improves over KNN and why that might be.\n",
        "\n",
        "\n",
        "Note: Precision, recall, F1 scores can be returned for both the classes. For the assignment we stick to the single-scalar version focused on mines. For precision, recall calculate with respect to the positive class denoted by class label 1. Note the variables denoting the test splits can change based on what you have used.\n",
        "\n",
        "```\n",
        "prec = precision_score(y_te, y_pred, pos_label=1)\n",
        "rec  = recall_score(y_te, y_pred, pos_label=1)\n",
        "f1   = f1_score(y_te, y_pred, pos_label=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1  Exploratory Check\n",
        "\n",
        "- Missing values  Report the count of NaN values in the raw CSV.\n",
        "\n",
        "- Class balance: (a) Display a bar plot of R vs M counts.\n",
        "\n",
        "(b) Compute the imbalance ratio (larger‑class ∕ smaller‑class).\n",
        "\n",
        "(c) Decide whether the dataset is balanced or imbalanced (justify). This is upto your discretion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Freq_1</th>\n",
              "      <th>Freq_2</th>\n",
              "      <th>Freq_3</th>\n",
              "      <th>Freq_4</th>\n",
              "      <th>Freq_5</th>\n",
              "      <th>Freq_6</th>\n",
              "      <th>Freq_7</th>\n",
              "      <th>Freq_8</th>\n",
              "      <th>Freq_9</th>\n",
              "      <th>Freq_10</th>\n",
              "      <th>...</th>\n",
              "      <th>Freq_52</th>\n",
              "      <th>Freq_53</th>\n",
              "      <th>Freq_54</th>\n",
              "      <th>Freq_55</th>\n",
              "      <th>Freq_56</th>\n",
              "      <th>Freq_57</th>\n",
              "      <th>Freq_58</th>\n",
              "      <th>Freq_59</th>\n",
              "      <th>Freq_60</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2328</td>\n",
              "      <td>0.2684</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0199</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1030</td>\n",
              "      <td>0.2154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.0522</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.1171</td>\n",
              "      <td>0.1257</td>\n",
              "      <td>0.1178</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.2529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.1354</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.1123</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0338</td>\n",
              "      <td>0.0655</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>0.1843</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows × 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Freq_1  Freq_2  Freq_3  Freq_4  Freq_5  Freq_6  Freq_7  Freq_8  Freq_9  \\\n",
              "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
              "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
              "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
              "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
              "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
              "\n",
              "     Freq_10  ...  Freq_52  Freq_53  Freq_54  Freq_55  Freq_56  Freq_57  \\\n",
              "0     0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   \n",
              "1     0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   \n",
              "2     0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   \n",
              "3     0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   \n",
              "4     0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   \n",
              "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
              "203   0.2684  ...   0.0116   0.0098   0.0199   0.0033   0.0101   0.0065   \n",
              "204   0.2154  ...   0.0061   0.0093   0.0135   0.0063   0.0063   0.0034   \n",
              "205   0.2529  ...   0.0160   0.0029   0.0051   0.0062   0.0089   0.0140   \n",
              "206   0.2354  ...   0.0086   0.0046   0.0126   0.0036   0.0035   0.0034   \n",
              "207   0.2354  ...   0.0146   0.0129   0.0047   0.0039   0.0061   0.0040   \n",
              "\n",
              "     Freq_58  Freq_59  Freq_60  Label  \n",
              "0     0.0084   0.0090   0.0032      R  \n",
              "1     0.0049   0.0052   0.0044      R  \n",
              "2     0.0164   0.0095   0.0078      R  \n",
              "3     0.0044   0.0040   0.0117      R  \n",
              "4     0.0048   0.0107   0.0094      R  \n",
              "..       ...      ...      ...    ...  \n",
              "203   0.0115   0.0193   0.0157      M  \n",
              "204   0.0032   0.0062   0.0067      M  \n",
              "205   0.0138   0.0077   0.0031      M  \n",
              "206   0.0079   0.0036   0.0048      M  \n",
              "207   0.0036   0.0061   0.0115      M  \n",
              "\n",
              "[208 rows x 61 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"sonar.all-data.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            "Freq_1     0\n",
            "Freq_2     0\n",
            "Freq_3     0\n",
            "Freq_4     0\n",
            "Freq_5     0\n",
            "          ..\n",
            "Freq_57    0\n",
            "Freq_58    0\n",
            "Freq_59    0\n",
            "Freq_60    0\n",
            "Label      0\n",
            "Length: 61, dtype: int64\n",
            "All NaN Values: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"All NaN Values:\", df.isna().sum().sum()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imbalance Ratio: 1.14\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y = df['Label'].astype('category').cat.codes # M = 1 (mine), R = 0 (rock)\n",
        "\n",
        "# Display bar plot of R vs M counts\n",
        "df['Label'].value_counts().plot(kind='bar', title='Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "class_counts = y.value_counts()\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This imbalance ratio is somewhat even. Although there is data on mines than rocks, it is not heavily skewed like an imbalance ratio of 2.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Part 2  Modelling\n",
        "\n",
        "- Perform 80 / 20 train‑test split (stratified if imbalanced).\n",
        "\n",
        "- use StandardScaler → KNeighborsClassifier inside each fold.\n",
        "\n",
        "- 5‑fold CV (use StratifiedKFold when imbalanced, plain KFold otherwise).\n",
        "\n",
        "- Evaluate two models: Model  (a) KNN n_neighbors = 3 (b) KNN n_neighbors = 5\n",
        "\n",
        "- For each model record:\n",
        "\n",
        "i) Mean training accuracy (across folds)\n",
        "\n",
        "ii) Mean validation accuracy\n",
        "\n",
        "iii) Gap = train − val\n",
        "\n",
        "- Test accuracy on the held‑out 20 % split\n",
        "\n",
        "- Confusion matrix on the test set\n",
        "\n",
        "- Precision, recall, F1  \n",
        "\n",
        "Fill the required metrics into the table template below (extend it with the new columns for precision/recall/F1):\n",
        "\n",
        "| k | Train Acc | Val Acc | Gap | Test Acc | Precision | Recall | F1 | Confusion Matrix | Best |\n",
        "| :-: | :-------: | :-----: | :--: | :------: | :-------: | :----: | :---: | :--------------- | :--: |\n",
        "\n",
        "Q1: State the best model whichever model you would deploy if the main goal is to minimise missed mines (maximise recall).\n",
        "\n",
        "Make your judgement using the following reasong - If Based on your validation accuracy for k = 3 looked best, but the hold-out test shows k = 5 has higher balanced accuracy. Which would you deploy given that missing a mine carries greater risk than a false alarm?”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn.model_selection\n",
        "\n",
        "# 80/20 train-test split\n",
        "X = df.drop(\"Label\", axis=1)\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.8, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Scalar KNN\n",
        "As the imbalance ratio isn't too high, I decided to use a standard scalar KNN. Had the imbalance ratio been 1.5 or greater, I would have used a stratifiedKFold instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold  \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def evaluate_knn(k):\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for train_index, val_index in kf.split(X_train):\n",
        "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "        model = KNeighborsClassifier(n_neighbors=k)\n",
        "        model.fit(X_tr_scaled, y_tr)\n",
        "\n",
        "        train_preds = model.predict(X_tr_scaled)\n",
        "        val_preds = model.predict(X_val_scaled)\n",
        "\n",
        "        train_accs.append(accuracy_score(y_tr, train_preds))\n",
        "        val_accs.append(accuracy_score(y_val, val_preds))\n",
        "\n",
        "    mean_train_acc = np.mean(train_accs)\n",
        "    mean_val_acc = np.mean(val_accs)\n",
        "    gap = mean_train_acc - mean_val_acc\n",
        "\n",
        "    # Final model on full training set\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    final_model = KNeighborsClassifier(n_neighbors=k)\n",
        "    final_model.fit(X_train_scaled, y_train)\n",
        "    test_preds = final_model.predict(X_test_scaled)\n",
        "\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    prec = precision_score(y_test, test_preds, pos_label=1)\n",
        "    rec = recall_score(y_test, test_preds, pos_label=1)\n",
        "    f1 = f1_score(y_test, test_preds, pos_label=1)\n",
        "    conf_mat = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "    return {\n",
        "        \"k\": k,\n",
        "        \"Train Acc\": round(mean_train_acc, 4),\n",
        "        \"Val Acc\": round(mean_val_acc, 4),\n",
        "        \"Gap\": round(gap, 4),\n",
        "        \"Test Acc\": round(test_acc, 4),\n",
        "        \"Precision\": round(prec, 4),\n",
        "        \"Recall\": round(rec, 4),\n",
        "        \"F1\": round(f1, 4),\n",
        "        \"Confusion Matrix\": conf_mat,\n",
        "    }\n",
        "\n",
        "# Continue running the rest of the code as before\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| k | Train Acc | Val Acc | Gap | Test Acc | Precision | Recall | F1 | Confusion Matrix | Best |\n",
            "| 3 | 0.9247 | 0.795 | 0.1297 | 0.9524 | 0.8889 | 1.0 | 0.9412 | [[24, 2], [0, 16]] | ✅ |\n",
            "| 5 | 0.8855 | 0.7765 | 0.1091 | 0.9048 | 0.8333 | 0.9375 | 0.8824 | [[23, 3], [1, 15]] |  |\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run evaluations\n",
        "results = [evaluate_knn(k) for k in [3, 5]]\n",
        "\n",
        "# Display results\n",
        "print(f\"| k | Train Acc | Val Acc | Gap | Test Acc | Precision | Recall | F1 | Best |\")\n",
        "# print(f\"|:-:|:----------:|:--------:|:---:|:---------:|:----------:|:------:|:----:|:------------------:|:----:|\")\n",
        "\n",
        "best_model = None\n",
        "best_recall = 0\n",
        "\n",
        "for result in results:\n",
        "    is_best = \"\"\n",
        "    if result[\"Recall\"] > best_recall:\n",
        "        best_model = result\n",
        "        best_recall = result[\"Recall\"]\n",
        "        is_best = \"✅\"\n",
        "    \n",
        "    print(f\"| {result['k']} | {result['Train Acc']} | {result['Val Acc']} | {result['Gap']} | {result['Test Acc']} | {result['Precision']} | {result['Recall']} | {result['F1']} | {is_best} |\")\n",
        "    \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
